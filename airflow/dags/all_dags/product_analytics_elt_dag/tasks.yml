embulk_dump_tables:
  for_each: embulk_dump_tables
  task_id: dump_{table_name}_to_s3
  operator: plugins.embulk_operator.EmbulkEcsOperator
  aws_conn_id: aws_conn
  cluster: "{{ var.value.ecs_cluster }}"
  awslogs_group: indicium-ecs-logs/embulk
  awslogs_region: "{{ var.value.aws_region }}"
  awslogs_stream_prefix: embulk/embulk
  launch_type: FARGATE
  pool: airflow_extraction_pool
  network_configuration:
    awsvpcConfiguration:
      subnets: ["{{ var.value.subnet1_id }}"] 
      securityGroups: ["{{ var.value.indicium_ecs_sg_id }}"] 
  config:
    in:
      table: "{table_name}"
      type: "postgresql"
      host: "{{ conn.airflow_metastore.host }}"
      port: "{{ conn.airflow_metastore.port }}"
      user: "{{ conn.airflow_metastore.login }}"
      password: "{{ conn.airflow_metastore.password }}"
      database: "{{ conn.airflow_metastore.schema }}"
    out:
      type: s3
      bucket: "{{ var.value.target_bucket }}"
      path_prefix: "{{ var.value.target_bucket_prefix }}/airflow_monitoring/{table_name}/{{ ds_nodash }}/"
      file_ext: ".json.gz"
      access_key_id: "{{ conn.aws_conn.login }}"
      secret_access_key: "{{conn.aws_conn.password}}"
      formatter: {"type": "jsonl"}
      encoders: [{"type": "gzip", "level": "1"}]

copy_s3_to_dw:
  for_each: copy_s3_dw_tables
  task_id: copy_{table_name}_s3_to_dw
  operator: airflow.operators.postgres_operator.PostgresOperator
  dependencies: ["dump_{table_name}_to_s3"]
  pool: airflow_extraction_pool
  sql: "./all_dags/product_analytics_elt_daily_dag/sql/copy_replace_truncate.sql"
  database: "project_dw"
  email_on_retry: false
  postgres_conn_id: project_redshift    
  params:
    schema: "raw_airflow_monitoring" 
    table_name: "{table_name}"
    s3_bucket_key: "airflow_monitoring/{table_name}"
    copy_options: json 'auto' gzip

run_anyway_product_analytics:
  task_id: "run_anyway_product_analytics"
  operator: airflow.operators.dummy_operator.DummyOperator
  trigger_rule: all_done
  dependencies: 
    - copy_dag_s3_to_dw
    - copy_dag_run_s3_to_dw
    - copy_task_instance_s3_to_dw
    - copy_task_fail_s3_to_dw

execute_dbt_tasks: 
  for_each: dbt_tasks
  task_id: "{task}"
  task_definition: project_dbt_dw
  operator: airflow.providers.amazon.aws.operators.ecs.EcsOperator
  dependencies: ["run_anyway_product_analytics"]
  aws_conn_id: aws_conn
  cluster: "{{ var.value.ecs_cluster }}"
  awslogs_group: indicium-ecs-logs/project-airflow-prod
  awslogs_region: "{{ var.value.aws_region }}"
  awslogs_stream_prefix: dbt/project_dbt_dw
  region: "{{ var.value.aws_region }}"
  launch_type: FARGATE
  pool: default_pool
  overrides: 
    containerOverrides: 
      - name: project_dbt_dw
        environment:
              - name: DBT_USER
                value: "{{ var.value.DW_USER }}"
              - name: DBT_PASSWORD
                value: "{{ var.value.DW_PASSWORD }}" 
  network_configuration:
    awsvpcConfiguration:
      subnets: ["{{ var.value.subnet1_id }}"] 
      securityGroups: ["{{ var.value.indicium_ecs_sg_id }}"]
      assignPublicIp: ENABLED